{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the IS-Academia website's *interesting* design choices, sacrificing a few goats to the deity of your choice may be required to fully understand this code.\n",
    "\n",
    "In the immortal words of Dante Alighieri: **Lasciate ogni speranza, voi ch'entrate!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the libs we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests # HTTP requests\n",
    "from bs4 import BeautifulSoup # HTML parsing\n",
    "import re # Regular expressions :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's get the index form, i.e. the base page we'll use to get all data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It's just an URL with a few weird symbols, how complex can it be?\n",
    "index_url = \"http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.filter\"\n",
    "\n",
    "# ...requesting that doesn't return anything, turns out we need URL parameters.\n",
    "# Surely it's simple and self-descriptive?\n",
    "index_params = {\n",
    "    # ...oh well...\n",
    "    \"ww_i_reportmodel\": \"133685247\" \n",
    "}\n",
    "\n",
    "index_html = requests.get(index_url, params=index_params).text\n",
    "index_page = BeautifulSoup(index_html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, fetch the page containing all of the \"Informatique\" (CS) links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the parameters for that page by fetching all of the \"hidden\" parameters, then adding our own to select CS.\n",
    "info_index_params = dict([(i[\"name\"], i[\"value\"]) for i in index_page.findAll(\"input\", attrs={\"type\": \"hidden\"})])\n",
    "\n",
    "# Find the \"HTML\" option, get its value.\n",
    "# Ideally we'd also find its name by looking for \"HTML\", but ISA doesn't use radiobuttons like normal people do,\n",
    "# instead they have a button then some text right next to it, so looking for \"html\" will just find the text. :/\n",
    "info_index_params[\"ww_i_reportModelXsl\"] = index_page.find(\"input\", attrs={\"name\": \"ww_i_reportModelXsl\"})[\"value\"]\n",
    "\n",
    "# Find the \"Informatique\" option, get its value\n",
    "info_index_params[\"ww_x_UNITE_ACAD\"] = index_page.find(\"option\", text=\"Informatique\")[\"value\"]\n",
    "\n",
    "info_index_html = requests.get(index_url, params=info_index_params).text\n",
    "info_index_page = BeautifulSoup(info_index_html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the page in a browser, there are links.  \n",
    "Have you ever heard of links? You define them with text, and an URL the user will go to if they click on the link.\n",
    "\n",
    "...well, that's how normal people do links.  \n",
    "IS-Academia does links that lead to nowhere, with JavaScript intercepting the click, creating an URL by manually scanning the user input on the page, setting a nested webpage's URL to that, and then reloading the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_semesters(name_regex):\n",
    "    \"\"\"Find all semesters in Informatique matching the given regex.\"\"\"\n",
    "    semesters_by_id = []\n",
    "    for link in info_index_page.findAll(\"a\", attrs={\"class\": \"ww_x_GPS\"}):\n",
    "        # Parse the link name, to find the year + semester\n",
    "        # The name regex is parenthesized so it's saved as group and not just matched\n",
    "        link_name_match = re.search(\"Informatique, (\\d+)-\\d+, (\" + name_regex + \")\", link.text)\n",
    "        \n",
    "        # Ignore weird stuff\n",
    "        if link_name_match is None:\n",
    "            continue\n",
    "        \n",
    "        # Find the link ID inside the onclick JavaScript.\n",
    "        # ...\n",
    "        # ...\n",
    "        # ...why? just... why? why would anybody ever write a webpage like this?\n",
    "        link_id_match = re.search(r\"ww_x_GPS=(\\d+)\", link[\"onclick\"])\n",
    "\n",
    "        semesters_by_id.append((link_id_match.group(1), link_name_match.group(1), link_name_match.group(2)))\n",
    "           \n",
    "    # Now filter them to keep only 2007 and onwards.\n",
    "    # Also remove those 2017 and later, since that hasn't happened yet so the data would be of dubious value.\n",
    "    semesters_by_id = [v for v in semesters_by_id if 2007 <= int(v[1]) <= 2016]\n",
    "    \n",
    "    # Are you scared yet? Now it gets worse!\n",
    "    students_by_semester = []\n",
    "    for semester in semesters_by_id:\n",
    "        # For some reason the URL is now .html instead of .filter\n",
    "        semester_url = \"http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.html\"\n",
    "        \n",
    "        # The parameters are the same as before, except now there's ww_x_GPS for the semester ID\n",
    "        semester_params = info_index_params.copy()\n",
    "        semester_params[\"ww_x_GPS\"] = semester[0]\n",
    "        \n",
    "        semester_html = requests.get(semester_url, params=semester_params).text\n",
    "        semester_page = BeautifulSoup(semester_html, \"lxml\")\n",
    "        \n",
    "        students = []\n",
    "        # Iterate all rows, except the ones that have headers\n",
    "        for row in semester_page.findAll(\"tr\"):\n",
    "            if row.contents[0].name != \"th\":\n",
    "                # Just get the name!\n",
    "                # It's the 2nd column. Can't be that hard.\n",
    "                student_name = row.contents[1].text\n",
    "                \n",
    "                # ...oh wait. Instead of a normal space, it's a non-breaking space.\n",
    "                # So let's replace that...\n",
    "                student_name = student_name.replace(\"\\xa0\", \" \")\n",
    "                \n",
    "                students.append(student_name)\n",
    "        \n",
    "        # Aaand we're done. Finally.\n",
    "        students_by_semester.append((semester[1], semester[2], students))\n",
    "        \n",
    "    return students_by_semester"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
